{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "from raptor_pack.llama_index.packs.raptor.base import RaptorRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from llama_index.core.schema import Document\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "nest_asyncio.apply()\n",
    "with open('hotpotqa_corpus.json') as file:\n",
    "\tdata = file.read()\n",
    "\tlines = json.loads(data)\n",
    "\n",
    "\n",
    "output_directory = 'hotpot_temp_data'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "all_file, count = [], 0\n",
    "for key in lines.keys():\n",
    "    file_path = os.path.join(output_directory, f\"{count}.txt\")\n",
    "    file_str = key + '\\n'\n",
    "    for sentence in lines[key]:\n",
    "         file_str += sentence\n",
    "    with open(file_path, 'w') as file:\n",
    "         file.write(file_str)\n",
    "    all_file.append(file_path)\n",
    "    count += 1\n",
    "\n",
    "corpus = json.load(open(\"hotpotqa_kg.json\"))\n",
    "documents = []\n",
    "entities_facts = {}\n",
    "fact_counts = {}\n",
    "for doc in corpus:\n",
    "    for rel in doc[\"facts\"]:\n",
    "        documents.append(Document(text=rel[\"fact\"]))\n",
    "        for ent in rel[\"entities\"]:\n",
    "            if ent.lower() not in entities_facts:\n",
    "                entities_facts[ent.lower()] = []\n",
    "            entities_facts[ent.lower()] += [rel[\"fact\"]]\n",
    "            if rel[\"fact\"] not in fact_counts:\n",
    "                fact_counts[rel[\"fact\"]] = 0\n",
    "            fact_counts[rel[\"fact\"]] += 1\n",
    "new_docs = set()\n",
    "for ent in entities_facts:\n",
    "    if len(entities_facts[ent]) == 1 and fact_counts[entities_facts[ent][0]] > 1:\n",
    "        continue\n",
    "    new_docs.add(\"\\n\".join(entities_facts[ent]))\n",
    "higher_level_facts = []\n",
    "for doc in new_docs:\n",
    "     higher_level_facts.append(Document(text=doc))\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=all_file).load_data()\n",
    "retriever = RaptorRetriever(documents, higher_level_facts=higher_level_facts, embed_model=OpenAIEmbedding(model=\"text-embedding-3-small\"), llm=OpenAI(model=\"gpt-4o\", temperature=0), similarity_top_k=20, mode=\"collapsed\", verbose = True)\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, llm=OpenAI(model=\"gpt-4o\", temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "with open('hotpotqa.json') as file:\n",
    "\tdata = file.read()\n",
    "\tlines2 = json.loads(data)\n",
    "\n",
    "execution_time = 0\n",
    "total_eval = []\n",
    "for value in lines2:\n",
    "    final_question = value['question'] + \" Answer this question in as fewer number of words as possible.\"\n",
    "    start_time = time.time()\n",
    "    response = query_engine.query(final_question)\n",
    "    end_time = time.time()\n",
    "    execution_time = execution_time + (end_time - start_time)\n",
    "    element = {\"q\": value['question'], \"a\": value['answer']}\n",
    "    element[\"predict\"] = str(response).strip()\n",
    "    total_eval.append(element)\n",
    "    print(\"Finished a file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/output_hotpot_SiReRAG_gpt4o_temp0.json', 'w') as file:\n",
    "    file.write(json.dumps(total_eval))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
